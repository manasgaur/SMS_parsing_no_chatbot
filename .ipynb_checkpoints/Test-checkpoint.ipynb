{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['withdraw', 'rs', 'indbnk', 'vk-boiind', 'creditcard', 'boi', 'vm-amex', 'icici bank', 'bob', 'bz-scisms', 'axis', 'imps', 'vk-fromsc', 'dm-yesbank', 'bt-indbnk', 'vm-sbiacs', 'bh-boiind', 'dm-rblbnk', 'vm-kotakb', 'vm-yesbank', 'icici', 'vk-icici', 'dm-bobtxn', 'dm-amex', 'vk-amexindia', 'vm-unionb', 'vm-hsbc', 'netbanking', 'hdfc', 'rtgs', 'vk-hsbcinhsbc', 'vk-sbi', 'a/c no.', 'vk-hsbcin', 'vm-icici', 'dm-bobind', 'credited', 'american express', 'vm-hdfcbk', 'bh-idbi', 'dm-icicib', 'vm-axis', 'tm-idbi', 'bp-atmsbi', 'online payment', 'axis bank', 'ad-rblbnk', 'transaction', 'kotakb', 'vk-hsbc', 'vm-fromsc', 'bp-yesbank', 'vm-axisb', 'vk-axisb', 'vk-amex', 'vk-unionb', 'hsbcin', 'inr', 'scind', 'vk-bobrwd', 'tm-sbiacs', 'vm-bobsms', 'vm-hsbcinhsbc', 'vm-bobtxn', 'vm-amexind', 'am-indbnk', 'bz-sbiacs', 'vk-axis', 'vk-rblbnk', 'withdrawn', 'icicib', 'tm-unionb', 'axis mobile', 'dz-fromsc', 'axisbank', 'rblbnk', 'vk-kotak', 'vm-rblbnk', 'idbi', 'dm-icici', 'dm-boiind', 'dm-amexind', 'kotak', 'dm-idbi', 'bx-atmsbi', 'hsbc', 'vk-sbiacs', 'vm-hdfc', 'vm-axisbk', 'rm-bobtxn', 'usd', 'hsbcinhsbc', 'rm-unionb', 'vk-kotakb', 'dm-hbcbk', 'vm-icicib', 'amex', 'bz-atmsbi', 'bz-sbiinb', 'vk-hdfcbk', 'bx-sbiacs', 'recharge', 'vm-hdfcmp', 'neft', 'bz-boiind', 'am-hdfc', 'vk-hdfc', 'payment', 'am-yesbank', 'vk-icicib', 'am-rblbnk', 'credit card', 'vm-kotak', 'vm-idbi', 'vm-boiind', 'vm-bobind', 'bobind', 'bz-idbi', 'unionb', 'rm-yesbank', 'vk-idbi', 'dm-fromsc', 'rm-bobind', 'debited', 'vk-axisbk', 'dh-fromsc', 'vk-yesbank']\n",
      "Data Preparation begin ..... \n",
      "\n",
      "Data Prepared. Pipeline begins.... \n",
      "\n",
      "Creating the testing dataset\n",
      "\n",
      "Creating the training dataset \n",
      "\n",
      "Creating the Tf-IDF model and Bag of words Model\n",
      "---- Performing Lemmatization and Simple NLP task of text preprocessing\n",
      "Tf-IDF, Bag of words model created.\n",
      "Performing Cross Validation......\n",
      "Model training starts...\n",
      "Model has been trained\n",
      "Testing the model\n",
      "Pipeline completed!!\n",
      "0.870606411742 0.880125195618\n",
      "training accuracy: 0.870606411742 \tTesting Accuracy: 0.880125195618\n",
      "Are you satisfied with the model? (y/n) (case-sensitive)y\n",
      "Do you want to save the model for future use ? (y/n) (case-sensitive) y\n",
      " Saving the machine learning model ....\n",
      " Machine learning model saved. Saving BOW and tfidf model .....\n",
      " BOW and Tf-idf model have been saved. Now you can use them using \"using pickle.load()\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle,csv,sys,os,itertools\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "import codecs,string\n",
    "import Tweet_tokenizer as tt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import  accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "import warnings\n",
    "\n",
    "def custom_warning_message(msg, *a):\n",
    "    return str(msg) + '\\n'\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "def detect_language(character):\n",
    "    maxchar = max(character)\n",
    "    if u'\\u0900' <= maxchar <= u'\\u097f':\n",
    "        return 'hindi'\n",
    "\n",
    "def lemmatization(message):\n",
    "    ## making all the messages of the same format ( small caps)\n",
    "    message = unicode(message, 'utf-8').lower()\n",
    "    words =  tt.tokenize(message)\n",
    "    temp=list()\n",
    "    for word in words:\n",
    "        if detect_language(word) == 'hindi':\n",
    "            return -1\n",
    "        else:\n",
    "            temp.append(word.encode('utf-8'))\n",
    "    return ' '.join(word for word in temp)\n",
    "\n",
    "def lemmatization_for_CV(message):\n",
    "    message = unicode(message, 'utf-8').lower()\n",
    "    words =  tt.tokenize(message)\n",
    "    return [wnl.lemmatize(word.encode('utf-8')) for word in words]\n",
    "\n",
    "def tfidf_BOW_model(training_dataset, vocab):\n",
    "    print \"---- Performing Lemmatization and Simple NLP task of text preprocessing\"\n",
    "    BOW_model = CountVectorizer(analyzer=lemmatization_for_CV, vocabulary=vocab).fit(training_dataset.message)\n",
    "    BOW_of_messages = BOW_model.transform(training_dataset.message)\n",
    "    tfidf = TfidfTransformer().fit(BOW_of_messages)\n",
    "    tfidf_over_train = tfidf.transform(BOW_of_messages)\n",
    "    print \"Tf-IDF, Bag of words model created.\"\n",
    "    return BOW_model, tfidf, tfidf_over_train\n",
    "\n",
    "def model_training(vectors, label):\n",
    "    classifier = RandomForestClassifier(n_estimators=14, random_state=15435).fit(vectors, label)\n",
    "    return classifier\n",
    "\n",
    "def model_testing(text_message, tfidf, BOW_model, classifier):\n",
    "    Test_BOW_message = BOW_model.transform(text_message)\n",
    "    test_tfidf_over_BOW_of_message = tfidf.transform(Test_BOW_message)\n",
    "    test_prediction = classifier.predict(test_tfidf_over_BOW_of_message)\n",
    "    return test_prediction\n",
    "\n",
    "def model_pipeline(final_dataset, vocab):\n",
    "    print \"Creating the testing dataset\\n\"\n",
    "    testing_dataset = final_dataset.sample(frac=0.3, replace=True)\n",
    "    print \"Creating the training dataset \\n\"\n",
    "    training_dataset = pd.concat([final_dataset, testing_dataset, testing_dataset]).drop_duplicates(keep=False)\n",
    "    print \"Creating the Tf-IDF model and Bag of words Model\"\n",
    "    BOW_model, tfidf, tfidf_over_train = tfidf_BOW_model(training_dataset, vocab)\n",
    "    print \"Performing Cross Validation......\"\n",
    "    TrmsgV,ValmsgV,Trlbl,Vallbl = train_test_split(tfidf_over_train, training_dataset.label, test_size=0.33)\n",
    "    print \"Model training starts...\"\n",
    "    classifier = model_training(TrmsgV, Trlbl)\n",
    "    print \"Model has been trained\"\n",
    "    validation_set_label_prediction = classifier.predict(ValmsgV)\n",
    "    training_accuracy = accuracy_score(Vallbl, validation_set_label_prediction)\n",
    "    print \"Testing the model\"\n",
    "    predictions = model_testing(testing_dataset.message, tfidf, BOW_model, classifier)\n",
    "    testing_accuracy = accuracy_score(testing_dataset.label,predictions)\n",
    "    print \"Pipeline completed!!\"\n",
    "    return training_accuracy, testing_accuracy, classifier, BOW_model, tfidf\n",
    "\n",
    "def gen_keywords_list (filename):\n",
    "    suspects_keywords_frame = pd.read_csv(filename)\n",
    "    positive_message_keywords = suspects_keywords_frame[suspects_keywords_frame['type'] == 'POSITIVE'].keyword.values\n",
    "    valid_message_sender = suspects_keywords_frame[suspects_keywords_frame['type'] == 'VALID SENDER'].keyword.values\n",
    "    return itertools.chain(positive_message_keywords, valid_message_sender)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    arg1 = 'suspect-keywords-20171017-131042.csv'\n",
    "    arg2 = 'suspect-messages-20171017-131005.csv'\n",
    "    while(1):\n",
    "        if not arg1:\n",
    "            if not arg2:\n",
    "                print \"Please enter suspect message csv file\"\n",
    "                raise FileNotFoundError\n",
    "            else:\n",
    "                print \"You have not entered the updated keywords file\"\n",
    "                input = raw_input(\"Do you want the system to use existing csv of keywords? enter y/n (case-sensitive)\")\n",
    "                if input == 'y':\n",
    "                    obj = gen_keywords_list('suspect-keywords-20171017-131042.csv')\n",
    "                    break\n",
    "                else:\n",
    "                    input = raw_input(\"enter the keywords file along with extension (.csv)\")\n",
    "                    obj = gen_keywords_list(input)\n",
    "                    break\n",
    "        else:\n",
    "            if not arg2:\n",
    "                print \"Please enter suspect message csv file\"\n",
    "                raise FileNotFoundError\n",
    "            else:\n",
    "                obj = gen_keywords_list(arg1)\n",
    "                break\n",
    "    \n",
    "    vocab = list(obj); vocab = list(set(vocab))\n",
    "    vocab = [v.lower() for v in vocab]\n",
    "    print vocab\n",
    "    suspect_message_frame = pd.read_csv(arg2)\n",
    "    suspect_message_frame = suspect_message_frame.sample(frac=1).reset_index(drop=True)\n",
    "    ab_actual_dataset = pd.DataFrame(columns=[\"message\", \"label\"])\n",
    "    ab_actual_dataset.message = suspect_message_frame.message.copy(deep=True)\n",
    "    ab_actual_dataset.label = suspect_message_frame.processed.copy(deep=True)\n",
    "    temp = list(ab_actual_dataset.label)\n",
    "    for item in temp:\n",
    "        if item == False:\n",
    "            temp.remove(item)\n",
    "            temp.append(0)\n",
    "        else:\n",
    "            temp.remove(item)\n",
    "            temp.append(1)\n",
    "    temp[0] = 1\n",
    "    ab_actual_dataset.label = temp\n",
    "    final_dataset = pd.DataFrame(columns=['message', 'label'])\n",
    "    print \"Data Preparation begin ..... \\n\"\n",
    "    for index, row in ab_actual_dataset.iterrows():\n",
    "        if lemmatization(row.message) == -1:\n",
    "            continue\n",
    "        else:\n",
    "            final_dataset = final_dataset.append({'message':lemmatization(row.message), 'label':row.label},ignore_index=True)\n",
    "    print \"Data Prepared. Pipeline begins.... \\n\"\n",
    "    final_dataset = shuffle(final_dataset)\n",
    "    Tr_score, Te_score, created_model, bow_model, tfidf_model  = model_pipeline(final_dataset, vocab)\n",
    "    print Tr_score, Te_score\n",
    "    print \"training accuracy:\", Tr_score,\"\\t\", \"Testing Accuracy:\", Te_score\n",
    "    user_input = raw_input(\"Are you satisfied with the model? (y/n) (case-sensitive)\")\n",
    "    if user_input == 'y':\n",
    "        re_input = raw_input(\"Do you want to save the model for future use ? (y/n) (case-sensitive) \")\n",
    "        if re_input == 'y':\n",
    "            model_name = 'machine_learning.sav'\n",
    "            bowmodel_name = 'bow_model.sav'\n",
    "            tfidfmodel_name = 'tfidf_model.sav'\n",
    "            print \" Saving the machine learning model ....\"\n",
    "            pickle.dump(created_model, open('trained_model/%s' %(model_name), 'wb'))\n",
    "            print \" Machine learning model saved. Saving BOW and tfidf model .....\"\n",
    "            pickle.dump(bow_model, open('trained_model/%s' %(bowmodel_name), 'wb'))\n",
    "            pickle.dump(tfidf_model, open('trained_model/%s' %(tfidfmodel_name),'wb'))\n",
    "            pickle.dump(vocab, open('vocab.sav','wb'))\n",
    "            print \" BOW and Tf-idf model have been saved. Now you can use them using \\\"using pickle.load()\\\"\"\n",
    "        else:\n",
    "            warnings.formatwarning = custom_warning_message\n",
    "            warnings.warn(\"model deleted!!!!\")\n",
    "    else:\n",
    "        warnings.formatwarning = custom_warning_message\n",
    "        warnings.warn(\"model deleted!!!!\")\n",
    "    #except FileNotFoundError:\n",
    "        #print \"Unable to locate the file. Please check the name of the file or its location.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "def lemmatization_for_CV(message):\n",
    "    message = unicode(message, 'utf-8').lower()\n",
    "    words =  tt.tokenize(message)\n",
    "    return [wnl.lemmatize(word) for word in words]\n",
    "\n",
    "bmodel = pickle.load(open('trained_model/bow_model.sav', 'rb'))\n",
    "tmodel = pickle.load(open('trained_model/tfidf_model.sav', 'rb'))\n",
    "message = \"An amount of Rs.8,258.05 has been debited from your a/c no XXXX9356 for BillPay/Credit Card payment done using HDFC Bank NetBanking\"\n",
    "print tmodel.transform(bmodel.transform(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer=<function lemmatization_for_CV at 0x10790c9b0>,\n",
      "        binary=False, decode_error=u'strict', dtype=<type 'numpy.int64'>,\n",
      "        encoding=u'utf-8', input=u'content', lowercase=True, max_df=1.0,\n",
      "        max_features=None, min_df=1, ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=None, strip_accents=None,\n",
      "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
      "        vocabulary=['Withdraw', 'Rs', 'INDBNK', 'VK-BOIIND', 'Creditcard', 'BOI', 'VM-AMEX', 'ICICI BANK', 'BOB', 'BZ-SCISMS', 'AXIS', 'IMPS', 'VK-FROMSC', 'DM-YESBANK', 'BT-INDBNK', 'VM-SBIACS', 'BH-BOIIND', 'DM-RBLBNK', 'VM-KOTAKB', 'VM-YESBANK', 'ICICI', 'VK-ICICI', 'DM-BOBTXN', 'DM-AMEX', 'VK-AMEXINDIA'...RM-YESBANK', 'VK-IDBI', 'DM-FROMSC', 'RM-BOBIND', 'debited', 'VK-AXISBK', 'DH-FROMSC', 'VK-YESBANK'])\n"
     ]
    }
   ],
   "source": [
    "print bmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
